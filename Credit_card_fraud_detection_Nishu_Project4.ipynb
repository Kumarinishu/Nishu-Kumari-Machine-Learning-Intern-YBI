{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Credit-card-fraud-detection-Nishu-Project4.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNiajsz8ZMzcl43863V0UdW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Kumarinishu/Nishu-Kumari-Machine-Learning-Intern-YBI/blob/main/Credit_card_fraud_detection_Nishu_Project4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Author: **NISHU KUMARI**\n",
        "# YBI FOUNDATION '**MACHINE LEARNING INTERN**'\n",
        "\n",
        "### Batch: 18th July\n",
        "\n",
        "### Project4: Credit Card Fraud Detection using Multiple Algorithm of Machine Learning like- SVM, KNN and ANN\n",
        "### Dataset Link: https://www.dropbox.com/s/6qcgvoc6h8y8zb2/CreditCardDefault.csv?dl=1"
      ],
      "metadata": {
        "id": "Yx3FaYBpKSeQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Importing Libraries"
      ],
      "metadata": {
        "id": "b6T_5nJhLCjq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "uQSYy5gKnOK6"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import accuracy_score, plot_confusion_matrix, classification_report,confusion_matrix"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loading the dataset"
      ],
      "metadata": {
        "id": "tRyC11IQLGzv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df=pd.read_csv('/content/sample_data/CreditCardDefault.csv')\n",
        "df.head(10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 456
        },
        "id": "d00xKQSyoY4_",
        "outputId": "61cbec57-ae72-4c9d-c242-1d8b3f020ca9"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   Time        V1        V2        V3        V4        V5        V6        V7  \\\n",
              "0     0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388  0.239599   \n",
              "1     0  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361 -0.078803   \n",
              "2     1 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499  0.791461   \n",
              "3     1 -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203  0.237609   \n",
              "4     2 -1.158233  0.877737  1.548718  0.403034 -0.407193  0.095921  0.592941   \n",
              "5     2 -0.425966  0.960523  1.141109 -0.168252  0.420987 -0.029728  0.476201   \n",
              "6     4  1.229658  0.141004  0.045371  1.202613  0.191881  0.272708 -0.005159   \n",
              "7     7 -0.644269  1.417964  1.074380 -0.492199  0.948934  0.428118  1.120631   \n",
              "8     7 -0.894286  0.286157 -0.113192 -0.271526  2.669599  3.721818  0.370145   \n",
              "9     9 -0.338262  1.119593  1.044367 -0.222187  0.499361 -0.246761  0.651583   \n",
              "\n",
              "         V8        V9  ...       V21       V22       V23       V24       V25  \\\n",
              "0  0.098698  0.363787  ... -0.018307  0.277838 -0.110474  0.066928  0.128539   \n",
              "1  0.085102 -0.255425  ... -0.225775 -0.638672  0.101288 -0.339846  0.167170   \n",
              "2  0.247676 -1.514654  ...  0.247998  0.771679  0.909412 -0.689281 -0.327642   \n",
              "3  0.377436 -1.387024  ... -0.108300  0.005274 -0.190321 -1.175575  0.647376   \n",
              "4 -0.270533  0.817739  ... -0.009431  0.798278 -0.137458  0.141267 -0.206010   \n",
              "5  0.260314 -0.568671  ... -0.208254 -0.559825 -0.026398 -0.371427 -0.232794   \n",
              "6  0.081213  0.464960  ... -0.167716 -0.270710 -0.154104 -0.780055  0.750137   \n",
              "7 -3.807864  0.615375  ...  1.943465 -1.015455  0.057504 -0.649709 -0.415267   \n",
              "8  0.851084 -0.392048  ... -0.073425 -0.268092 -0.204233  1.011592  0.373205   \n",
              "9  0.069539 -0.736727  ... -0.246914 -0.633753 -0.120794 -0.385050 -0.069733   \n",
              "\n",
              "        V26       V27       V28  Amount  Class  \n",
              "0 -0.189115  0.133558 -0.021053  149.62    0.0  \n",
              "1  0.125895 -0.008983  0.014724    2.69    0.0  \n",
              "2 -0.139097 -0.055353 -0.059752  378.66    0.0  \n",
              "3 -0.221929  0.062723  0.061458  123.50    0.0  \n",
              "4  0.502292  0.219422  0.215153   69.99    0.0  \n",
              "5  0.105915  0.253844  0.081080    3.67    0.0  \n",
              "6 -0.257237  0.034507  0.005168    4.99    0.0  \n",
              "7 -0.051634 -1.206921 -1.085339   40.80    0.0  \n",
              "8 -0.384157  0.011747  0.142404   93.20    0.0  \n",
              "9  0.094199  0.246219  0.083076    3.68    0.0  \n",
              "\n",
              "[10 rows x 31 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-fbfddbc5-cf53-4958-ac40-b4b18e89cc0e\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Time</th>\n",
              "      <th>V1</th>\n",
              "      <th>V2</th>\n",
              "      <th>V3</th>\n",
              "      <th>V4</th>\n",
              "      <th>V5</th>\n",
              "      <th>V6</th>\n",
              "      <th>V7</th>\n",
              "      <th>V8</th>\n",
              "      <th>V9</th>\n",
              "      <th>...</th>\n",
              "      <th>V21</th>\n",
              "      <th>V22</th>\n",
              "      <th>V23</th>\n",
              "      <th>V24</th>\n",
              "      <th>V25</th>\n",
              "      <th>V26</th>\n",
              "      <th>V27</th>\n",
              "      <th>V28</th>\n",
              "      <th>Amount</th>\n",
              "      <th>Class</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>-1.359807</td>\n",
              "      <td>-0.072781</td>\n",
              "      <td>2.536347</td>\n",
              "      <td>1.378155</td>\n",
              "      <td>-0.338321</td>\n",
              "      <td>0.462388</td>\n",
              "      <td>0.239599</td>\n",
              "      <td>0.098698</td>\n",
              "      <td>0.363787</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.018307</td>\n",
              "      <td>0.277838</td>\n",
              "      <td>-0.110474</td>\n",
              "      <td>0.066928</td>\n",
              "      <td>0.128539</td>\n",
              "      <td>-0.189115</td>\n",
              "      <td>0.133558</td>\n",
              "      <td>-0.021053</td>\n",
              "      <td>149.62</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>1.191857</td>\n",
              "      <td>0.266151</td>\n",
              "      <td>0.166480</td>\n",
              "      <td>0.448154</td>\n",
              "      <td>0.060018</td>\n",
              "      <td>-0.082361</td>\n",
              "      <td>-0.078803</td>\n",
              "      <td>0.085102</td>\n",
              "      <td>-0.255425</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.225775</td>\n",
              "      <td>-0.638672</td>\n",
              "      <td>0.101288</td>\n",
              "      <td>-0.339846</td>\n",
              "      <td>0.167170</td>\n",
              "      <td>0.125895</td>\n",
              "      <td>-0.008983</td>\n",
              "      <td>0.014724</td>\n",
              "      <td>2.69</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>-1.358354</td>\n",
              "      <td>-1.340163</td>\n",
              "      <td>1.773209</td>\n",
              "      <td>0.379780</td>\n",
              "      <td>-0.503198</td>\n",
              "      <td>1.800499</td>\n",
              "      <td>0.791461</td>\n",
              "      <td>0.247676</td>\n",
              "      <td>-1.514654</td>\n",
              "      <td>...</td>\n",
              "      <td>0.247998</td>\n",
              "      <td>0.771679</td>\n",
              "      <td>0.909412</td>\n",
              "      <td>-0.689281</td>\n",
              "      <td>-0.327642</td>\n",
              "      <td>-0.139097</td>\n",
              "      <td>-0.055353</td>\n",
              "      <td>-0.059752</td>\n",
              "      <td>378.66</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>-0.966272</td>\n",
              "      <td>-0.185226</td>\n",
              "      <td>1.792993</td>\n",
              "      <td>-0.863291</td>\n",
              "      <td>-0.010309</td>\n",
              "      <td>1.247203</td>\n",
              "      <td>0.237609</td>\n",
              "      <td>0.377436</td>\n",
              "      <td>-1.387024</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.108300</td>\n",
              "      <td>0.005274</td>\n",
              "      <td>-0.190321</td>\n",
              "      <td>-1.175575</td>\n",
              "      <td>0.647376</td>\n",
              "      <td>-0.221929</td>\n",
              "      <td>0.062723</td>\n",
              "      <td>0.061458</td>\n",
              "      <td>123.50</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2</td>\n",
              "      <td>-1.158233</td>\n",
              "      <td>0.877737</td>\n",
              "      <td>1.548718</td>\n",
              "      <td>0.403034</td>\n",
              "      <td>-0.407193</td>\n",
              "      <td>0.095921</td>\n",
              "      <td>0.592941</td>\n",
              "      <td>-0.270533</td>\n",
              "      <td>0.817739</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.009431</td>\n",
              "      <td>0.798278</td>\n",
              "      <td>-0.137458</td>\n",
              "      <td>0.141267</td>\n",
              "      <td>-0.206010</td>\n",
              "      <td>0.502292</td>\n",
              "      <td>0.219422</td>\n",
              "      <td>0.215153</td>\n",
              "      <td>69.99</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>2</td>\n",
              "      <td>-0.425966</td>\n",
              "      <td>0.960523</td>\n",
              "      <td>1.141109</td>\n",
              "      <td>-0.168252</td>\n",
              "      <td>0.420987</td>\n",
              "      <td>-0.029728</td>\n",
              "      <td>0.476201</td>\n",
              "      <td>0.260314</td>\n",
              "      <td>-0.568671</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.208254</td>\n",
              "      <td>-0.559825</td>\n",
              "      <td>-0.026398</td>\n",
              "      <td>-0.371427</td>\n",
              "      <td>-0.232794</td>\n",
              "      <td>0.105915</td>\n",
              "      <td>0.253844</td>\n",
              "      <td>0.081080</td>\n",
              "      <td>3.67</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>4</td>\n",
              "      <td>1.229658</td>\n",
              "      <td>0.141004</td>\n",
              "      <td>0.045371</td>\n",
              "      <td>1.202613</td>\n",
              "      <td>0.191881</td>\n",
              "      <td>0.272708</td>\n",
              "      <td>-0.005159</td>\n",
              "      <td>0.081213</td>\n",
              "      <td>0.464960</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.167716</td>\n",
              "      <td>-0.270710</td>\n",
              "      <td>-0.154104</td>\n",
              "      <td>-0.780055</td>\n",
              "      <td>0.750137</td>\n",
              "      <td>-0.257237</td>\n",
              "      <td>0.034507</td>\n",
              "      <td>0.005168</td>\n",
              "      <td>4.99</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>7</td>\n",
              "      <td>-0.644269</td>\n",
              "      <td>1.417964</td>\n",
              "      <td>1.074380</td>\n",
              "      <td>-0.492199</td>\n",
              "      <td>0.948934</td>\n",
              "      <td>0.428118</td>\n",
              "      <td>1.120631</td>\n",
              "      <td>-3.807864</td>\n",
              "      <td>0.615375</td>\n",
              "      <td>...</td>\n",
              "      <td>1.943465</td>\n",
              "      <td>-1.015455</td>\n",
              "      <td>0.057504</td>\n",
              "      <td>-0.649709</td>\n",
              "      <td>-0.415267</td>\n",
              "      <td>-0.051634</td>\n",
              "      <td>-1.206921</td>\n",
              "      <td>-1.085339</td>\n",
              "      <td>40.80</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>7</td>\n",
              "      <td>-0.894286</td>\n",
              "      <td>0.286157</td>\n",
              "      <td>-0.113192</td>\n",
              "      <td>-0.271526</td>\n",
              "      <td>2.669599</td>\n",
              "      <td>3.721818</td>\n",
              "      <td>0.370145</td>\n",
              "      <td>0.851084</td>\n",
              "      <td>-0.392048</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.073425</td>\n",
              "      <td>-0.268092</td>\n",
              "      <td>-0.204233</td>\n",
              "      <td>1.011592</td>\n",
              "      <td>0.373205</td>\n",
              "      <td>-0.384157</td>\n",
              "      <td>0.011747</td>\n",
              "      <td>0.142404</td>\n",
              "      <td>93.20</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>9</td>\n",
              "      <td>-0.338262</td>\n",
              "      <td>1.119593</td>\n",
              "      <td>1.044367</td>\n",
              "      <td>-0.222187</td>\n",
              "      <td>0.499361</td>\n",
              "      <td>-0.246761</td>\n",
              "      <td>0.651583</td>\n",
              "      <td>0.069539</td>\n",
              "      <td>-0.736727</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.246914</td>\n",
              "      <td>-0.633753</td>\n",
              "      <td>-0.120794</td>\n",
              "      <td>-0.385050</td>\n",
              "      <td>-0.069733</td>\n",
              "      <td>0.094199</td>\n",
              "      <td>0.246219</td>\n",
              "      <td>0.083076</td>\n",
              "      <td>3.68</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>10 rows × 31 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-fbfddbc5-cf53-4958-ac40-b4b18e89cc0e')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-fbfddbc5-cf53-4958-ac40-b4b18e89cc0e button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-fbfddbc5-cf53-4958-ac40-b4b18e89cc0e');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Processing & Understanding"
      ],
      "metadata": {
        "id": "6cGrQ_PSLTPq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zns9uvftqRog",
        "outputId": "8aa11a56-ae81-4907-96a3-fe925c2755bf"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2923, 31)"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.columns"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zgwsreU0qaFS",
        "outputId": "5eadfb75-36e3-4a25-e5a0-c9d462f0e615"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['Time', 'V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10',\n",
              "       'V11', 'V12', 'V13', 'V14', 'V15', 'V16', 'V17', 'V18', 'V19', 'V20',\n",
              "       'V21', 'V22', 'V23', 'V24', 'V25', 'V26', 'V27', 'V28', 'Amount',\n",
              "       'Class'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p0Fjwo20ql26",
        "outputId": "ee5ca339-1331-41b2-de10-fba99c7452f5"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 2923 entries, 0 to 2922\n",
            "Data columns (total 31 columns):\n",
            " #   Column  Non-Null Count  Dtype  \n",
            "---  ------  --------------  -----  \n",
            " 0   Time    2923 non-null   int64  \n",
            " 1   V1      2923 non-null   float64\n",
            " 2   V2      2923 non-null   float64\n",
            " 3   V3      2923 non-null   float64\n",
            " 4   V4      2923 non-null   float64\n",
            " 5   V5      2923 non-null   float64\n",
            " 6   V6      2923 non-null   float64\n",
            " 7   V7      2923 non-null   float64\n",
            " 8   V8      2923 non-null   float64\n",
            " 9   V9      2923 non-null   float64\n",
            " 10  V10     2923 non-null   float64\n",
            " 11  V11     2923 non-null   float64\n",
            " 12  V12     2923 non-null   float64\n",
            " 13  V13     2923 non-null   float64\n",
            " 14  V14     2923 non-null   float64\n",
            " 15  V15     2923 non-null   float64\n",
            " 16  V16     2923 non-null   float64\n",
            " 17  V17     2923 non-null   float64\n",
            " 18  V18     2923 non-null   float64\n",
            " 19  V19     2923 non-null   float64\n",
            " 20  V20     2923 non-null   float64\n",
            " 21  V21     2923 non-null   float64\n",
            " 22  V22     2923 non-null   float64\n",
            " 23  V23     2923 non-null   float64\n",
            " 24  V24     2923 non-null   float64\n",
            " 25  V25     2923 non-null   float64\n",
            " 26  V26     2923 non-null   float64\n",
            " 27  V27     2923 non-null   float64\n",
            " 28  V28     2922 non-null   float64\n",
            " 29  Amount  2922 non-null   float64\n",
            " 30  Class   2922 non-null   float64\n",
            "dtypes: float64(30), int64(1)\n",
            "memory usage: 708.0 KB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.describe(include='all')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 393
        },
        "id": "usy36iOFtm-H",
        "outputId": "e92de417-4747-4ea1-8843-692bfe030979"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "              Time           V1           V2           V3           V4  \\\n",
              "count  2923.000000  2923.000000  2923.000000  2923.000000  2923.000000   \n",
              "mean   1157.248375    -0.334017     0.295477     0.845482     0.102388   \n",
              "std     704.290301     1.393153     1.223387     0.986023     1.324235   \n",
              "min       0.000000   -12.168192   -15.732974   -12.389545    -4.657545   \n",
              "25%     550.000000    -1.088167    -0.180548     0.290367    -0.762449   \n",
              "50%    1135.000000    -0.458705     0.357519     0.868256     0.140817   \n",
              "75%    1712.000000     1.086080     0.950014     1.455047     1.002457   \n",
              "max    2471.000000     1.685314     6.118940     4.017561     6.013346   \n",
              "\n",
              "                V5           V6           V7           V8           V9  ...  \\\n",
              "count  2923.000000  2923.000000  2923.000000  2923.000000  2923.000000  ...   \n",
              "mean     -0.084588     0.041536     0.125553    -0.078615     0.039314  ...   \n",
              "std       1.237915     1.277126     1.121500     1.147635     0.912087  ...   \n",
              "min     -32.092129    -7.465603    -8.945496   -19.176657    -3.110515  ...   \n",
              "25%      -0.603171    -0.714916    -0.311558    -0.197142    -0.479566  ...   \n",
              "50%      -0.140202    -0.209263     0.122413     0.028133     0.010309  ...   \n",
              "75%       0.374203     0.421043     0.593711     0.287463     0.541219  ...   \n",
              "max      10.658654    21.393069    34.303177     3.877662     6.450992  ...   \n",
              "\n",
              "               V21          V22          V23          V24          V25  \\\n",
              "count  2923.000000  2923.000000  2923.000000  2923.000000  2923.000000   \n",
              "mean      0.022294    -0.140314    -0.042219     0.013856     0.100449   \n",
              "std       0.833485     0.618648     0.354303     0.595526     0.409030   \n",
              "min      -4.709977    -4.432106    -4.020300    -2.162523    -1.577384   \n",
              "25%      -0.225683    -0.547154    -0.187037    -0.358373    -0.148071   \n",
              "50%      -0.080783    -0.143518    -0.058762     0.089264     0.118704   \n",
              "75%       0.089006     0.284397     0.070922     0.423843     0.370447   \n",
              "max      14.718212     1.957759     4.095021     1.215279     1.629684   \n",
              "\n",
              "               V26          V27          V28       Amount        Class  \n",
              "count  2923.000000  2923.000000  2922.000000  2922.000000  2922.000000  \n",
              "mean      0.031599     0.045141     0.001299    67.955986     0.000684  \n",
              "std       0.478827     0.357938     0.265467   235.093423     0.026158  \n",
              "min      -1.243924    -5.336289    -2.909294     0.000000     0.000000  \n",
              "25%      -0.302827    -0.042663    -0.017615     3.817500     0.000000  \n",
              "50%       0.021763     0.029386     0.022927    13.990000     0.000000  \n",
              "75%       0.307106     0.184856     0.093151    56.965000     0.000000  \n",
              "max       3.463246     3.852046     4.157934  7712.430000     1.000000  \n",
              "\n",
              "[8 rows x 31 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-c41cfdbf-386b-4759-8f53-07b5fb63122d\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Time</th>\n",
              "      <th>V1</th>\n",
              "      <th>V2</th>\n",
              "      <th>V3</th>\n",
              "      <th>V4</th>\n",
              "      <th>V5</th>\n",
              "      <th>V6</th>\n",
              "      <th>V7</th>\n",
              "      <th>V8</th>\n",
              "      <th>V9</th>\n",
              "      <th>...</th>\n",
              "      <th>V21</th>\n",
              "      <th>V22</th>\n",
              "      <th>V23</th>\n",
              "      <th>V24</th>\n",
              "      <th>V25</th>\n",
              "      <th>V26</th>\n",
              "      <th>V27</th>\n",
              "      <th>V28</th>\n",
              "      <th>Amount</th>\n",
              "      <th>Class</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>2923.000000</td>\n",
              "      <td>2923.000000</td>\n",
              "      <td>2923.000000</td>\n",
              "      <td>2923.000000</td>\n",
              "      <td>2923.000000</td>\n",
              "      <td>2923.000000</td>\n",
              "      <td>2923.000000</td>\n",
              "      <td>2923.000000</td>\n",
              "      <td>2923.000000</td>\n",
              "      <td>2923.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>2923.000000</td>\n",
              "      <td>2923.000000</td>\n",
              "      <td>2923.000000</td>\n",
              "      <td>2923.000000</td>\n",
              "      <td>2923.000000</td>\n",
              "      <td>2923.000000</td>\n",
              "      <td>2923.000000</td>\n",
              "      <td>2922.000000</td>\n",
              "      <td>2922.000000</td>\n",
              "      <td>2922.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>1157.248375</td>\n",
              "      <td>-0.334017</td>\n",
              "      <td>0.295477</td>\n",
              "      <td>0.845482</td>\n",
              "      <td>0.102388</td>\n",
              "      <td>-0.084588</td>\n",
              "      <td>0.041536</td>\n",
              "      <td>0.125553</td>\n",
              "      <td>-0.078615</td>\n",
              "      <td>0.039314</td>\n",
              "      <td>...</td>\n",
              "      <td>0.022294</td>\n",
              "      <td>-0.140314</td>\n",
              "      <td>-0.042219</td>\n",
              "      <td>0.013856</td>\n",
              "      <td>0.100449</td>\n",
              "      <td>0.031599</td>\n",
              "      <td>0.045141</td>\n",
              "      <td>0.001299</td>\n",
              "      <td>67.955986</td>\n",
              "      <td>0.000684</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>704.290301</td>\n",
              "      <td>1.393153</td>\n",
              "      <td>1.223387</td>\n",
              "      <td>0.986023</td>\n",
              "      <td>1.324235</td>\n",
              "      <td>1.237915</td>\n",
              "      <td>1.277126</td>\n",
              "      <td>1.121500</td>\n",
              "      <td>1.147635</td>\n",
              "      <td>0.912087</td>\n",
              "      <td>...</td>\n",
              "      <td>0.833485</td>\n",
              "      <td>0.618648</td>\n",
              "      <td>0.354303</td>\n",
              "      <td>0.595526</td>\n",
              "      <td>0.409030</td>\n",
              "      <td>0.478827</td>\n",
              "      <td>0.357938</td>\n",
              "      <td>0.265467</td>\n",
              "      <td>235.093423</td>\n",
              "      <td>0.026158</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>-12.168192</td>\n",
              "      <td>-15.732974</td>\n",
              "      <td>-12.389545</td>\n",
              "      <td>-4.657545</td>\n",
              "      <td>-32.092129</td>\n",
              "      <td>-7.465603</td>\n",
              "      <td>-8.945496</td>\n",
              "      <td>-19.176657</td>\n",
              "      <td>-3.110515</td>\n",
              "      <td>...</td>\n",
              "      <td>-4.709977</td>\n",
              "      <td>-4.432106</td>\n",
              "      <td>-4.020300</td>\n",
              "      <td>-2.162523</td>\n",
              "      <td>-1.577384</td>\n",
              "      <td>-1.243924</td>\n",
              "      <td>-5.336289</td>\n",
              "      <td>-2.909294</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>550.000000</td>\n",
              "      <td>-1.088167</td>\n",
              "      <td>-0.180548</td>\n",
              "      <td>0.290367</td>\n",
              "      <td>-0.762449</td>\n",
              "      <td>-0.603171</td>\n",
              "      <td>-0.714916</td>\n",
              "      <td>-0.311558</td>\n",
              "      <td>-0.197142</td>\n",
              "      <td>-0.479566</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.225683</td>\n",
              "      <td>-0.547154</td>\n",
              "      <td>-0.187037</td>\n",
              "      <td>-0.358373</td>\n",
              "      <td>-0.148071</td>\n",
              "      <td>-0.302827</td>\n",
              "      <td>-0.042663</td>\n",
              "      <td>-0.017615</td>\n",
              "      <td>3.817500</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>1135.000000</td>\n",
              "      <td>-0.458705</td>\n",
              "      <td>0.357519</td>\n",
              "      <td>0.868256</td>\n",
              "      <td>0.140817</td>\n",
              "      <td>-0.140202</td>\n",
              "      <td>-0.209263</td>\n",
              "      <td>0.122413</td>\n",
              "      <td>0.028133</td>\n",
              "      <td>0.010309</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.080783</td>\n",
              "      <td>-0.143518</td>\n",
              "      <td>-0.058762</td>\n",
              "      <td>0.089264</td>\n",
              "      <td>0.118704</td>\n",
              "      <td>0.021763</td>\n",
              "      <td>0.029386</td>\n",
              "      <td>0.022927</td>\n",
              "      <td>13.990000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>1712.000000</td>\n",
              "      <td>1.086080</td>\n",
              "      <td>0.950014</td>\n",
              "      <td>1.455047</td>\n",
              "      <td>1.002457</td>\n",
              "      <td>0.374203</td>\n",
              "      <td>0.421043</td>\n",
              "      <td>0.593711</td>\n",
              "      <td>0.287463</td>\n",
              "      <td>0.541219</td>\n",
              "      <td>...</td>\n",
              "      <td>0.089006</td>\n",
              "      <td>0.284397</td>\n",
              "      <td>0.070922</td>\n",
              "      <td>0.423843</td>\n",
              "      <td>0.370447</td>\n",
              "      <td>0.307106</td>\n",
              "      <td>0.184856</td>\n",
              "      <td>0.093151</td>\n",
              "      <td>56.965000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>2471.000000</td>\n",
              "      <td>1.685314</td>\n",
              "      <td>6.118940</td>\n",
              "      <td>4.017561</td>\n",
              "      <td>6.013346</td>\n",
              "      <td>10.658654</td>\n",
              "      <td>21.393069</td>\n",
              "      <td>34.303177</td>\n",
              "      <td>3.877662</td>\n",
              "      <td>6.450992</td>\n",
              "      <td>...</td>\n",
              "      <td>14.718212</td>\n",
              "      <td>1.957759</td>\n",
              "      <td>4.095021</td>\n",
              "      <td>1.215279</td>\n",
              "      <td>1.629684</td>\n",
              "      <td>3.463246</td>\n",
              "      <td>3.852046</td>\n",
              "      <td>4.157934</td>\n",
              "      <td>7712.430000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>8 rows × 31 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c41cfdbf-386b-4759-8f53-07b5fb63122d')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-c41cfdbf-386b-4759-8f53-07b5fb63122d button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-c41cfdbf-386b-4759-8f53-07b5fb63122d');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.isnull().sum()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D-aaeUCcqqfr",
        "outputId": "2fe590a3-9432-4d8a-f506-e653ad8bafe8"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Time      0\n",
              "V1        0\n",
              "V2        0\n",
              "V3        0\n",
              "V4        0\n",
              "V5        0\n",
              "V6        0\n",
              "V7        0\n",
              "V8        0\n",
              "V9        0\n",
              "V10       0\n",
              "V11       0\n",
              "V12       0\n",
              "V13       0\n",
              "V14       0\n",
              "V15       0\n",
              "V16       0\n",
              "V17       0\n",
              "V18       0\n",
              "V19       0\n",
              "V20       0\n",
              "V21       0\n",
              "V22       0\n",
              "V23       0\n",
              "V24       0\n",
              "V25       0\n",
              "V26       0\n",
              "V27       0\n",
              "V28       1\n",
              "Amount    1\n",
              "Class     1\n",
              "dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.dropna(inplace=True)"
      ],
      "metadata": {
        "id": "m81MiJvlsZRg"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.isnull().sum()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CCMWUYXcuDfc",
        "outputId": "f0623c41-96b7-47f5-c100-8b7063ab5f86"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Time      0\n",
              "V1        0\n",
              "V2        0\n",
              "V3        0\n",
              "V4        0\n",
              "V5        0\n",
              "V6        0\n",
              "V7        0\n",
              "V8        0\n",
              "V9        0\n",
              "V10       0\n",
              "V11       0\n",
              "V12       0\n",
              "V13       0\n",
              "V14       0\n",
              "V15       0\n",
              "V16       0\n",
              "V17       0\n",
              "V18       0\n",
              "V19       0\n",
              "V20       0\n",
              "V21       0\n",
              "V22       0\n",
              "V23       0\n",
              "V24       0\n",
              "V25       0\n",
              "V26       0\n",
              "V27       0\n",
              "V28       0\n",
              "Amount    0\n",
              "Class     0\n",
              "dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Visualization"
      ],
      "metadata": {
        "id": "d6kWwHoKLuhl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sns.countplot(x = df['Class'],color='yellow')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "id": "xk1G6m01uJQr",
        "outputId": "5bb5e11c-bae8-4a5b-aec6-8f9e25f5e9a0"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f5f3aaae6d0>"
            ]
          },
          "metadata": {},
          "execution_count": 10
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df['Class'].value_counts()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L6ZjD3QGuWW3",
        "outputId": "80ef97cd-6a84-4875-abb8-d9f8cf2a7dbf"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.0    2920\n",
              "1.0       2\n",
              "Name: Class, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sc = StandardScaler()\n",
        "amount = df['Amount'].values\n",
        "df['Amount'] = sc.fit_transform(amount.reshape(-1, 1))"
      ],
      "metadata": {
        "id": "02Fx2zsou5nK"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.drop(['Time'], axis=1, inplace=True)"
      ],
      "metadata": {
        "id": "416Ggb8av_nr"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Np-Tx-ooxJ4V",
        "outputId": "ec0d5323-e8ba-44f8-df6b-92a0e9806bc9"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2922, 30)"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.drop_duplicates(inplace=True)"
      ],
      "metadata": {
        "id": "JGswkirMxSHG"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iFrntpGSxXZK",
        "outputId": "f08b2eb8-6241-4f1f-8377-313aebd900bb"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2882, 30)"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train & Test Split"
      ],
      "metadata": {
        "id": "Kwr7u_rxLe2y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = df.drop('Class', axis = 1).values\n",
        "y = df['Class'].values"
      ],
      "metadata": {
        "id": "o8zT1GmFxejy"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mmh6Q2NKyy2C",
        "outputId": "4ecfcff4-344b-4ff2-ec46-046a082f9574"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2882, 29)"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2xKm5UtwyykB",
        "outputId": "3fe4a947-1594-49a0-9bfa-348198519be1"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2882,)"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.30, random_state = 12345)"
      ],
      "metadata": {
        "id": "1l8AcsHoxnFZ"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model Building"
      ],
      "metadata": {
        "id": "FUNgcTY_LnEm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Support Vector Machine(SVM)"
      ],
      "metadata": {
        "id": "8_Ryqo4UyDPK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import LinearSVC, LinearSVR, SVC, SVR\n",
        "svm = SVC()\n",
        "svm.fit(X_train, y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AXbbym6kxxNE",
        "outputId": "ff54e46e-b611-481f-a904-58d5520b4bee"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SVC()"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "svm_yhat = svm.predict(X_test)\n",
        "svm_yhat"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qeS7v4XMyc-g",
        "outputId": "8523d7e7-980a-43a6-91da-75e9f42321d8"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Checking the accuracy of our svm model."
      ],
      "metadata": {
        "id": "buL-cjSPMEF3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Checking the accuracy of the model\n",
        "score=accuracy_score(y_test,svm_yhat)\n",
        "print(\"Accuracy:\",score)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3tjDO27Gycyj",
        "outputId": "0eeec82b-5e7b-43bc-ff33-39a3ab724ccb"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def report(model):\n",
        "    preds=model.predict(X_test)\n",
        "    print(classification_report(preds,y_test))\n",
        "    plot_confusion_matrix(model,X_test,y_test,cmap='nipy_spectral',colorbar=True)"
      ],
      "metadata": {
        "id": "sWCmAf3_ycmD"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Checking the accuracy of our svm model and plotting confusion matrix."
      ],
      "metadata": {
        "id": "9I-z6XDMMPwQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('Support Vector Machine(SVM)')\n",
        "report(svm)\n",
        "print(f'Accuracy using Support Vector Machine(SVM): {round(score*100,2)}%')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 520
        },
        "id": "XPMe3x_kycPi",
        "outputId": "1551c20a-b60b-4c97-ea6a-c8104ba028ee"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Support Vector Machine(SVM)\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       1.00      1.00      1.00       865\n",
            "\n",
            "    accuracy                           1.00       865\n",
            "   macro avg       1.00      1.00      1.00       865\n",
            "weighted avg       1.00      1.00      1.00       865\n",
            "\n",
            "Accuracy using Support Vector Machine(SVM): 100.0%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function plot_confusion_matrix is deprecated; Function `plot_confusion_matrix` is deprecated in 1.0 and will be removed in 1.2. Use one of the class methods: ConfusionMatrixDisplay.from_predictions or ConfusionMatrixDisplay.from_estimator.\n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. k-nearest neighbors or knn"
      ],
      "metadata": {
        "id": "B_FT81ig3f3K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor\n",
        "n = 7\n",
        "KNN = KNeighborsClassifier(n_neighbors = n)\n",
        "KNN.fit(X_train, y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W4cTPn010vaf",
        "outputId": "ec41ab57-1290-4a33-9513-31bfdb1c8f24"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "KNeighborsClassifier(n_neighbors=7)"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "knn_yhat = KNN.predict(X_test)\n",
        "knn_yhat"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Vmwsrg26N6t",
        "outputId": "c1ab0185-f2e0-413e-f8ae-1c3dca75f263"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Checking the accuracy of our knn model."
      ],
      "metadata": {
        "id": "jWEOOpgXNKaE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Checking the accuracy of the model\n",
        "score2=accuracy_score(y_test,knn_yhat)\n",
        "print(\"Accuracy:\",score2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VrMqY9FO6QXB",
        "outputId": "0dcda217-09d3-435e-96f0-74b7c4f98b33"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Checking the accuracy of our knn model and plotting confusion matrix."
      ],
      "metadata": {
        "id": "tdQJQ6OiMeVc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('K-Nearest Neighbors(KNN)')\n",
        "report(KNN)\n",
        "print(f'Accuracy using K-Nearest Neighbors(KNN): {round(score2*100,2)}%')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 520
        },
        "id": "kzWTLlpY7OEO",
        "outputId": "16639606-1c27-4e44-e4af-7ae8fc0e8cb5"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "K-Nearest Neighbors(KNN)\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       1.00      1.00      1.00       865\n",
            "\n",
            "    accuracy                           1.00       865\n",
            "   macro avg       1.00      1.00      1.00       865\n",
            "weighted avg       1.00      1.00      1.00       865\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function plot_confusion_matrix is deprecated; Function `plot_confusion_matrix` is deprecated in 1.0 and will be removed in 1.2. Use one of the class methods: ConfusionMatrixDisplay.from_predictions or ConfusionMatrixDisplay.from_estimator.\n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy using K-Nearest Neighbors(KNN): 100.0%\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Artificial Neural Network or ANN"
      ],
      "metadata": {
        "id": "RntD9cgm9cY6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.drop(['Amount'],axis=1)\n",
        "df.head(10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 456
        },
        "id": "7k8LiiOfAY6w",
        "outputId": "45f4b1c2-b75e-4929-c5e5-e92268b93a46"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "         V1        V2        V3        V4        V5        V6        V7  \\\n",
              "0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388  0.239599   \n",
              "1  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361 -0.078803   \n",
              "2 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499  0.791461   \n",
              "3 -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203  0.237609   \n",
              "4 -1.158233  0.877737  1.548718  0.403034 -0.407193  0.095921  0.592941   \n",
              "5 -0.425966  0.960523  1.141109 -0.168252  0.420987 -0.029728  0.476201   \n",
              "6  1.229658  0.141004  0.045371  1.202613  0.191881  0.272708 -0.005159   \n",
              "7 -0.644269  1.417964  1.074380 -0.492199  0.948934  0.428118  1.120631   \n",
              "8 -0.894286  0.286157 -0.113192 -0.271526  2.669599  3.721818  0.370145   \n",
              "9 -0.338262  1.119593  1.044367 -0.222187  0.499361 -0.246761  0.651583   \n",
              "\n",
              "         V8        V9       V10  ...       V20       V21       V22       V23  \\\n",
              "0  0.098698  0.363787  0.090794  ...  0.251412 -0.018307  0.277838 -0.110474   \n",
              "1  0.085102 -0.255425 -0.166974  ... -0.069083 -0.225775 -0.638672  0.101288   \n",
              "2  0.247676 -1.514654  0.207643  ...  0.524980  0.247998  0.771679  0.909412   \n",
              "3  0.377436 -1.387024 -0.054952  ... -0.208038 -0.108300  0.005274 -0.190321   \n",
              "4 -0.270533  0.817739  0.753074  ...  0.408542 -0.009431  0.798278 -0.137458   \n",
              "5  0.260314 -0.568671 -0.371407  ...  0.084968 -0.208254 -0.559825 -0.026398   \n",
              "6  0.081213  0.464960 -0.099254  ... -0.219633 -0.167716 -0.270710 -0.154104   \n",
              "7 -3.807864  0.615375  1.249376  ... -0.156742  1.943465 -1.015455  0.057504   \n",
              "8  0.851084 -0.392048 -0.410430  ...  0.052736 -0.073425 -0.268092 -0.204233   \n",
              "9  0.069539 -0.736727 -0.366846  ...  0.203711 -0.246914 -0.633753 -0.120794   \n",
              "\n",
              "        V24       V25       V26       V27       V28  Class  \n",
              "0  0.066928  0.128539 -0.189115  0.133558 -0.021053    0.0  \n",
              "1 -0.339846  0.167170  0.125895 -0.008983  0.014724    0.0  \n",
              "2 -0.689281 -0.327642 -0.139097 -0.055353 -0.059752    0.0  \n",
              "3 -1.175575  0.647376 -0.221929  0.062723  0.061458    0.0  \n",
              "4  0.141267 -0.206010  0.502292  0.219422  0.215153    0.0  \n",
              "5 -0.371427 -0.232794  0.105915  0.253844  0.081080    0.0  \n",
              "6 -0.780055  0.750137 -0.257237  0.034507  0.005168    0.0  \n",
              "7 -0.649709 -0.415267 -0.051634 -1.206921 -1.085339    0.0  \n",
              "8  1.011592  0.373205 -0.384157  0.011747  0.142404    0.0  \n",
              "9 -0.385050 -0.069733  0.094199  0.246219  0.083076    0.0  \n",
              "\n",
              "[10 rows x 29 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-ed06f00c-b3bc-426c-b30e-918b0797386e\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>V1</th>\n",
              "      <th>V2</th>\n",
              "      <th>V3</th>\n",
              "      <th>V4</th>\n",
              "      <th>V5</th>\n",
              "      <th>V6</th>\n",
              "      <th>V7</th>\n",
              "      <th>V8</th>\n",
              "      <th>V9</th>\n",
              "      <th>V10</th>\n",
              "      <th>...</th>\n",
              "      <th>V20</th>\n",
              "      <th>V21</th>\n",
              "      <th>V22</th>\n",
              "      <th>V23</th>\n",
              "      <th>V24</th>\n",
              "      <th>V25</th>\n",
              "      <th>V26</th>\n",
              "      <th>V27</th>\n",
              "      <th>V28</th>\n",
              "      <th>Class</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-1.359807</td>\n",
              "      <td>-0.072781</td>\n",
              "      <td>2.536347</td>\n",
              "      <td>1.378155</td>\n",
              "      <td>-0.338321</td>\n",
              "      <td>0.462388</td>\n",
              "      <td>0.239599</td>\n",
              "      <td>0.098698</td>\n",
              "      <td>0.363787</td>\n",
              "      <td>0.090794</td>\n",
              "      <td>...</td>\n",
              "      <td>0.251412</td>\n",
              "      <td>-0.018307</td>\n",
              "      <td>0.277838</td>\n",
              "      <td>-0.110474</td>\n",
              "      <td>0.066928</td>\n",
              "      <td>0.128539</td>\n",
              "      <td>-0.189115</td>\n",
              "      <td>0.133558</td>\n",
              "      <td>-0.021053</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1.191857</td>\n",
              "      <td>0.266151</td>\n",
              "      <td>0.166480</td>\n",
              "      <td>0.448154</td>\n",
              "      <td>0.060018</td>\n",
              "      <td>-0.082361</td>\n",
              "      <td>-0.078803</td>\n",
              "      <td>0.085102</td>\n",
              "      <td>-0.255425</td>\n",
              "      <td>-0.166974</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.069083</td>\n",
              "      <td>-0.225775</td>\n",
              "      <td>-0.638672</td>\n",
              "      <td>0.101288</td>\n",
              "      <td>-0.339846</td>\n",
              "      <td>0.167170</td>\n",
              "      <td>0.125895</td>\n",
              "      <td>-0.008983</td>\n",
              "      <td>0.014724</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-1.358354</td>\n",
              "      <td>-1.340163</td>\n",
              "      <td>1.773209</td>\n",
              "      <td>0.379780</td>\n",
              "      <td>-0.503198</td>\n",
              "      <td>1.800499</td>\n",
              "      <td>0.791461</td>\n",
              "      <td>0.247676</td>\n",
              "      <td>-1.514654</td>\n",
              "      <td>0.207643</td>\n",
              "      <td>...</td>\n",
              "      <td>0.524980</td>\n",
              "      <td>0.247998</td>\n",
              "      <td>0.771679</td>\n",
              "      <td>0.909412</td>\n",
              "      <td>-0.689281</td>\n",
              "      <td>-0.327642</td>\n",
              "      <td>-0.139097</td>\n",
              "      <td>-0.055353</td>\n",
              "      <td>-0.059752</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-0.966272</td>\n",
              "      <td>-0.185226</td>\n",
              "      <td>1.792993</td>\n",
              "      <td>-0.863291</td>\n",
              "      <td>-0.010309</td>\n",
              "      <td>1.247203</td>\n",
              "      <td>0.237609</td>\n",
              "      <td>0.377436</td>\n",
              "      <td>-1.387024</td>\n",
              "      <td>-0.054952</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.208038</td>\n",
              "      <td>-0.108300</td>\n",
              "      <td>0.005274</td>\n",
              "      <td>-0.190321</td>\n",
              "      <td>-1.175575</td>\n",
              "      <td>0.647376</td>\n",
              "      <td>-0.221929</td>\n",
              "      <td>0.062723</td>\n",
              "      <td>0.061458</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-1.158233</td>\n",
              "      <td>0.877737</td>\n",
              "      <td>1.548718</td>\n",
              "      <td>0.403034</td>\n",
              "      <td>-0.407193</td>\n",
              "      <td>0.095921</td>\n",
              "      <td>0.592941</td>\n",
              "      <td>-0.270533</td>\n",
              "      <td>0.817739</td>\n",
              "      <td>0.753074</td>\n",
              "      <td>...</td>\n",
              "      <td>0.408542</td>\n",
              "      <td>-0.009431</td>\n",
              "      <td>0.798278</td>\n",
              "      <td>-0.137458</td>\n",
              "      <td>0.141267</td>\n",
              "      <td>-0.206010</td>\n",
              "      <td>0.502292</td>\n",
              "      <td>0.219422</td>\n",
              "      <td>0.215153</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>-0.425966</td>\n",
              "      <td>0.960523</td>\n",
              "      <td>1.141109</td>\n",
              "      <td>-0.168252</td>\n",
              "      <td>0.420987</td>\n",
              "      <td>-0.029728</td>\n",
              "      <td>0.476201</td>\n",
              "      <td>0.260314</td>\n",
              "      <td>-0.568671</td>\n",
              "      <td>-0.371407</td>\n",
              "      <td>...</td>\n",
              "      <td>0.084968</td>\n",
              "      <td>-0.208254</td>\n",
              "      <td>-0.559825</td>\n",
              "      <td>-0.026398</td>\n",
              "      <td>-0.371427</td>\n",
              "      <td>-0.232794</td>\n",
              "      <td>0.105915</td>\n",
              "      <td>0.253844</td>\n",
              "      <td>0.081080</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>1.229658</td>\n",
              "      <td>0.141004</td>\n",
              "      <td>0.045371</td>\n",
              "      <td>1.202613</td>\n",
              "      <td>0.191881</td>\n",
              "      <td>0.272708</td>\n",
              "      <td>-0.005159</td>\n",
              "      <td>0.081213</td>\n",
              "      <td>0.464960</td>\n",
              "      <td>-0.099254</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.219633</td>\n",
              "      <td>-0.167716</td>\n",
              "      <td>-0.270710</td>\n",
              "      <td>-0.154104</td>\n",
              "      <td>-0.780055</td>\n",
              "      <td>0.750137</td>\n",
              "      <td>-0.257237</td>\n",
              "      <td>0.034507</td>\n",
              "      <td>0.005168</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>-0.644269</td>\n",
              "      <td>1.417964</td>\n",
              "      <td>1.074380</td>\n",
              "      <td>-0.492199</td>\n",
              "      <td>0.948934</td>\n",
              "      <td>0.428118</td>\n",
              "      <td>1.120631</td>\n",
              "      <td>-3.807864</td>\n",
              "      <td>0.615375</td>\n",
              "      <td>1.249376</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.156742</td>\n",
              "      <td>1.943465</td>\n",
              "      <td>-1.015455</td>\n",
              "      <td>0.057504</td>\n",
              "      <td>-0.649709</td>\n",
              "      <td>-0.415267</td>\n",
              "      <td>-0.051634</td>\n",
              "      <td>-1.206921</td>\n",
              "      <td>-1.085339</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>-0.894286</td>\n",
              "      <td>0.286157</td>\n",
              "      <td>-0.113192</td>\n",
              "      <td>-0.271526</td>\n",
              "      <td>2.669599</td>\n",
              "      <td>3.721818</td>\n",
              "      <td>0.370145</td>\n",
              "      <td>0.851084</td>\n",
              "      <td>-0.392048</td>\n",
              "      <td>-0.410430</td>\n",
              "      <td>...</td>\n",
              "      <td>0.052736</td>\n",
              "      <td>-0.073425</td>\n",
              "      <td>-0.268092</td>\n",
              "      <td>-0.204233</td>\n",
              "      <td>1.011592</td>\n",
              "      <td>0.373205</td>\n",
              "      <td>-0.384157</td>\n",
              "      <td>0.011747</td>\n",
              "      <td>0.142404</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>-0.338262</td>\n",
              "      <td>1.119593</td>\n",
              "      <td>1.044367</td>\n",
              "      <td>-0.222187</td>\n",
              "      <td>0.499361</td>\n",
              "      <td>-0.246761</td>\n",
              "      <td>0.651583</td>\n",
              "      <td>0.069539</td>\n",
              "      <td>-0.736727</td>\n",
              "      <td>-0.366846</td>\n",
              "      <td>...</td>\n",
              "      <td>0.203711</td>\n",
              "      <td>-0.246914</td>\n",
              "      <td>-0.633753</td>\n",
              "      <td>-0.120794</td>\n",
              "      <td>-0.385050</td>\n",
              "      <td>-0.069733</td>\n",
              "      <td>0.094199</td>\n",
              "      <td>0.246219</td>\n",
              "      <td>0.083076</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>10 rows × 29 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ed06f00c-b3bc-426c-b30e-918b0797386e')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-ed06f00c-b3bc-426c-b30e-918b0797386e button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-ed06f00c-b3bc-426c-b30e-918b0797386e');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = df.iloc[:, df.columns != 'Class']\n",
        "Y = df.iloc[:, df.columns == 'Class']"
      ],
      "metadata": {
        "id": "_tTDbgRiAY2k"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Number of data points in the minority class\n",
        "number_records_fraud = len(df[df.Class == 1])\n",
        "fraud_indices = np.array(df[df.Class == 1].index)\n",
        "\n",
        "# Picking the indices of the normal classes\n",
        "normal_indices = df[df.Class == 0].index\n",
        "\n",
        "# Out of the indices we picked, randomly select \"x\" number (number_records_fraud)\n",
        "random_normal_indices = np.random.choice(normal_indices, number_records_fraud, replace = False)\n",
        "random_normal_indices = np.array(random_normal_indices)\n",
        "\n",
        "# Appending the 2 indices\n",
        "under_sample_indices = np.concatenate([fraud_indices,random_normal_indices])\n",
        "\n",
        "# Under sample dataset\n",
        "under_sample_data = df.iloc[under_sample_indices,:]\n",
        "\n",
        "X_undersample = under_sample_data.iloc[:, under_sample_data.columns != 'Class']\n",
        "y_undersample = under_sample_data.iloc[:, under_sample_data.columns == 'Class']\n",
        "\n",
        "# Showing ratio\n",
        "print(\"Percentage of normal transactions: \", len(under_sample_data[under_sample_data.Class == 0])/len(under_sample_data))\n",
        "print(\"Percentage of fraud transactions: \", len(under_sample_data[under_sample_data.Class == 1])/len(under_sample_data))\n",
        "print(\"Total number of transactions in resampled data: \", len(under_sample_data))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LlyOtQ0zAY1A",
        "outputId": "5ca6503b-e029-4c55-a9ac-a7755da2108e"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Percentage of normal transactions:  1.0\n",
            "Percentage of fraud transactions:  0.0\n",
            "Total number of transactions in resampled data:  4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Out of the indices we picked, randomly select \"x\" number (number_records_fraud)\n",
        "random_normal_indices = np.random.choice(normal_indices, number_records_fraud, replace = False)\n",
        "random_normal_indices = np.array(random_normal_indices)\n",
        "\n",
        "# Appending the 2 indices\n",
        "under_sample_indices = np.concatenate([fraud_indices,random_normal_indices])\n",
        "\n",
        "# Under sample dataset\n",
        "under_sample_data = df.iloc[under_sample_indices,:]\n",
        "\n",
        "X_undersample = under_sample_data.iloc[:, under_sample_data.columns != 'Class']\n",
        "y_undersample = under_sample_data.iloc[:, under_sample_data.columns == 'Class']\n",
        "\n",
        "# Showing ratio\n",
        "print(\"Percentage of normal transactions: \", len(under_sample_data[under_sample_data.Class == 0])/len(under_sample_data))\n",
        "print(\"Percentage of fraud transactions: \", len(under_sample_data[under_sample_data.Class == 1])/len(under_sample_data))\n",
        "print(\"Total number of transactions in resampled data: \", len(under_sample_data))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dmoszc3vAYwF",
        "outputId": "ebbaaa46-d254-4a02-ba58-d9021d0cb6a9"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Percentage of normal transactions:  1.0\n",
            "Percentage of fraud transactions:  0.0\n",
            "Total number of transactions in resampled data:  4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Whole dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(x,Y,test_size = 0.3, random_state = 18976)\n",
        "\n",
        "print(\"Number transactions train dataset: \", len(X_train))\n",
        "print(\"Number transactions test dataset: \", len(X_test))\n",
        "print(\"Total number of transactions: \", len(X_train)+len(X_test))\n",
        "\n",
        "# Undersampled dataset\n",
        "X_train_undersample, X_test_undersample, y_train_undersample, y_test_undersample = train_test_split(X_undersample\n",
        "                                                                                                   ,y_undersample\n",
        "                                                                                                   ,test_size = 0.3\n",
        "                                                                                                   ,random_state = 18976)\n",
        "print(\"\")\n",
        "print(\"Number transactions train dataset: \", len(X_train_undersample))\n",
        "print(\"Number transactions test dataset: \", len(X_test_undersample))\n",
        "print(\"Total number of transactions: \", len(X_train_undersample)+len(X_test_undersample))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UqQNjNk7AYul",
        "outputId": "b3fd50b5-b234-4a2e-8b7b-79c85c60d88a"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number transactions train dataset:  2017\n",
            "Number transactions test dataset:  865\n",
            "Total number of transactions:  2882\n",
            "\n",
            "Number transactions train dataset:  2\n",
            "Number transactions test dataset:  2\n",
            "Total number of transactions:  4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "scaled_X_train = sc.fit_transform(X_train_undersample)\n",
        "scaled_X_test = sc.fit_transform(X_test_undersample)"
      ],
      "metadata": {
        "id": "ZiEIkg5RAYqo"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_undersample.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nEL3aCP-AYo_",
        "outputId": "de6c3d2f-f875-4bd2-cb79-7c781b0d9deb"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2, 28)"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Checking the accuracy of our ann model and printing the report."
      ],
      "metadata": {
        "id": "CqbTkZOgMsCW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix , classification_report\n",
        "from tensorflow import keras\n",
        "def ANN(scaled_X_train, y_train, scaled_X_test, y_test, loss):\n",
        "    model = keras.Sequential([\n",
        "        keras.layers.Dense(28, input_dim=28, activation='relu'),\n",
        "        keras.layers.Dense(15, activation='relu'),\n",
        "        keras.layers.Dense(15, activation='relu'),\n",
        "        keras.layers.Dense(15, activation='relu'),\n",
        "        keras.layers.Dense(15, activation='relu'),\n",
        "        keras.layers.Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "\n",
        "    model.compile(optimizer='adam', loss=loss, metrics=['accuracy'])\n",
        "\n",
        "    \n",
        "    model.fit(scaled_X_train, y_train, epochs=100)\n",
        "\n",
        "    print(model.evaluate(scaled_X_test, y_test))\n",
        "\n",
        "    y_preds = model.predict(scaled_X_test)\n",
        "    y_preds = np.round(y_preds)\n",
        "\n",
        "    print(\"Classification Report: \\n\", classification_report(y_test, y_preds))\n",
        "\n",
        "    return y_preds"
      ],
      "metadata": {
        "id": "2JXqlc3I7rgB"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_preds = ANN(scaled_X_train, y_train_undersample, scaled_X_test, y_test_undersample, 'binary_crossentropy')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0ERkSkaDAYks",
        "outputId": "e50fa9fb-de5c-4f2a-90c8-00129976152c"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "1/1 [==============================] - 1s 975ms/step - loss: 0.4052 - accuracy: 1.0000\n",
            "Epoch 2/100\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.3791 - accuracy: 1.0000\n",
            "Epoch 3/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.3535 - accuracy: 1.0000\n",
            "Epoch 4/100\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.3284 - accuracy: 1.0000\n",
            "Epoch 5/100\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.3040 - accuracy: 1.0000\n",
            "Epoch 6/100\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.2809 - accuracy: 1.0000\n",
            "Epoch 7/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.2586 - accuracy: 1.0000\n",
            "Epoch 8/100\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.2369 - accuracy: 1.0000\n",
            "Epoch 9/100\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.2161 - accuracy: 1.0000\n",
            "Epoch 10/100\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.1961 - accuracy: 1.0000\n",
            "Epoch 11/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.1775 - accuracy: 1.0000\n",
            "Epoch 12/100\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.1600 - accuracy: 1.0000\n",
            "Epoch 13/100\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.1437 - accuracy: 1.0000\n",
            "Epoch 14/100\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.1284 - accuracy: 1.0000\n",
            "Epoch 15/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.1144 - accuracy: 1.0000\n",
            "Epoch 16/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.1014 - accuracy: 1.0000\n",
            "Epoch 17/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.0896 - accuracy: 1.0000\n",
            "Epoch 18/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.0789 - accuracy: 1.0000\n",
            "Epoch 19/100\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.0693 - accuracy: 1.0000\n",
            "Epoch 20/100\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.0607 - accuracy: 1.0000\n",
            "Epoch 21/100\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.0532 - accuracy: 1.0000\n",
            "Epoch 22/100\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.0465 - accuracy: 1.0000\n",
            "Epoch 23/100\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.0407 - accuracy: 1.0000\n",
            "Epoch 24/100\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.0356 - accuracy: 1.0000\n",
            "Epoch 25/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.0312 - accuracy: 1.0000\n",
            "Epoch 26/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.0274 - accuracy: 1.0000\n",
            "Epoch 27/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.0240 - accuracy: 1.0000\n",
            "Epoch 28/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.0212 - accuracy: 1.0000\n",
            "Epoch 29/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.0187 - accuracy: 1.0000\n",
            "Epoch 30/100\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.0165 - accuracy: 1.0000\n",
            "Epoch 31/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.0146 - accuracy: 1.0000\n",
            "Epoch 32/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.0130 - accuracy: 1.0000\n",
            "Epoch 33/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.0116 - accuracy: 1.0000\n",
            "Epoch 34/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.0104 - accuracy: 1.0000\n",
            "Epoch 35/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.0093 - accuracy: 1.0000\n",
            "Epoch 36/100\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.0084 - accuracy: 1.0000\n",
            "Epoch 37/100\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.0076 - accuracy: 1.0000\n",
            "Epoch 38/100\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.0069 - accuracy: 1.0000\n",
            "Epoch 39/100\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.0063 - accuracy: 1.0000\n",
            "Epoch 40/100\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.0058 - accuracy: 1.0000\n",
            "Epoch 41/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.0053 - accuracy: 1.0000\n",
            "Epoch 42/100\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.0049 - accuracy: 1.0000\n",
            "Epoch 43/100\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.0046 - accuracy: 1.0000\n",
            "Epoch 44/100\n",
            "1/1 [==============================] - 0s 23ms/step - loss: 0.0043 - accuracy: 1.0000\n",
            "Epoch 45/100\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.0040 - accuracy: 1.0000\n",
            "Epoch 46/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.0037 - accuracy: 1.0000\n",
            "Epoch 47/100\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.0035 - accuracy: 1.0000\n",
            "Epoch 48/100\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.0033 - accuracy: 1.0000\n",
            "Epoch 49/100\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.0031 - accuracy: 1.0000\n",
            "Epoch 50/100\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.0029 - accuracy: 1.0000\n",
            "Epoch 51/100\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.0028 - accuracy: 1.0000\n",
            "Epoch 52/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.0026 - accuracy: 1.0000\n",
            "Epoch 53/100\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.0025 - accuracy: 1.0000\n",
            "Epoch 54/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.0024 - accuracy: 1.0000\n",
            "Epoch 55/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.0023 - accuracy: 1.0000\n",
            "Epoch 56/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.0022 - accuracy: 1.0000\n",
            "Epoch 57/100\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.0021 - accuracy: 1.0000\n",
            "Epoch 58/100\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.0020 - accuracy: 1.0000\n",
            "Epoch 59/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.0019 - accuracy: 1.0000\n",
            "Epoch 60/100\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.0019 - accuracy: 1.0000\n",
            "Epoch 61/100\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.0018 - accuracy: 1.0000\n",
            "Epoch 62/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.0018 - accuracy: 1.0000\n",
            "Epoch 63/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.0017 - accuracy: 1.0000\n",
            "Epoch 64/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.0016 - accuracy: 1.0000\n",
            "Epoch 65/100\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.0016 - accuracy: 1.0000\n",
            "Epoch 66/100\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.0015 - accuracy: 1.0000\n",
            "Epoch 67/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.0015 - accuracy: 1.0000\n",
            "Epoch 68/100\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.0015 - accuracy: 1.0000\n",
            "Epoch 69/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.0014 - accuracy: 1.0000\n",
            "Epoch 70/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.0014 - accuracy: 1.0000\n",
            "Epoch 71/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.0014 - accuracy: 1.0000\n",
            "Epoch 72/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.0013 - accuracy: 1.0000\n",
            "Epoch 73/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.0013 - accuracy: 1.0000\n",
            "Epoch 74/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.0013 - accuracy: 1.0000\n",
            "Epoch 75/100\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.0012 - accuracy: 1.0000\n",
            "Epoch 76/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.0012 - accuracy: 1.0000\n",
            "Epoch 77/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.0012 - accuracy: 1.0000\n",
            "Epoch 78/100\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.0012 - accuracy: 1.0000\n",
            "Epoch 79/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.0011 - accuracy: 1.0000\n",
            "Epoch 80/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.0011 - accuracy: 1.0000\n",
            "Epoch 81/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.0011 - accuracy: 1.0000\n",
            "Epoch 82/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.0011 - accuracy: 1.0000\n",
            "Epoch 83/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.0010 - accuracy: 1.0000\n",
            "Epoch 84/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.0010 - accuracy: 1.0000\n",
            "Epoch 85/100\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.0010 - accuracy: 1.0000\n",
            "Epoch 86/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 9.8875e-04 - accuracy: 1.0000\n",
            "Epoch 87/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 9.7054e-04 - accuracy: 1.0000\n",
            "Epoch 88/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 9.5307e-04 - accuracy: 1.0000\n",
            "Epoch 89/100\n",
            "1/1 [==============================] - 0s 25ms/step - loss: 9.3613e-04 - accuracy: 1.0000\n",
            "Epoch 90/100\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 9.1970e-04 - accuracy: 1.0000\n",
            "Epoch 91/100\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 9.0375e-04 - accuracy: 1.0000\n",
            "Epoch 92/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 8.8826e-04 - accuracy: 1.0000\n",
            "Epoch 93/100\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 8.7322e-04 - accuracy: 1.0000\n",
            "Epoch 94/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 8.5859e-04 - accuracy: 1.0000\n",
            "Epoch 95/100\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 8.4436e-04 - accuracy: 1.0000\n",
            "Epoch 96/100\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 8.3052e-04 - accuracy: 1.0000\n",
            "Epoch 97/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 8.1705e-04 - accuracy: 1.0000\n",
            "Epoch 98/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 8.0394e-04 - accuracy: 1.0000\n",
            "Epoch 99/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 7.9116e-04 - accuracy: 1.0000\n",
            "Epoch 100/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 7.7871e-04 - accuracy: 1.0000\n",
            "1/1 [==============================] - 0s 252ms/step - loss: 0.0383 - accuracy: 1.0000\n",
            "[0.03833944350481033, 1.0]\n",
            "Classification Report: \n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "         0.0       1.00      1.00      1.00         2\n",
            "\n",
            "    accuracy                           1.00         2\n",
            "   macro avg       1.00      1.00      1.00         2\n",
            "weighted avg       1.00      1.00      1.00         2\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = []\n",
        "for element in y_preds:\n",
        "    if element > 0.5:\n",
        "        y_pred.append(1)\n",
        "    else:\n",
        "        y_pred.append(0)"
      ],
      "metadata": {
        "id": "i2mXmefOAYjH"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_preds[:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B9r3tM10AYap",
        "outputId": "e5c53b55-9c9e-4b2c-a08f-adb7f44eaf08"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.],\n",
              "       [0.]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred[:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TWyVcqtfAYY_",
        "outputId": "29ba126e-5eb5-4dad-a31a-f8e4ee82b9d8"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0, 0]"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "cm = tf.math.confusion_matrix(labels=y_test_undersample,predictions=y_pred)\n",
        "\n",
        "plt.figure(figsize = (10,7))\n",
        "sns.heatmap(cm, annot=True, fmt='d')\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Truth')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 464
        },
        "id": "tDeE154WAYP0",
        "outputId": "e9be6409-eeba-47c5-a96e-8fc1cf347779"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(69.0, 0.5, 'Truth')"
            ]
          },
          "metadata": {},
          "execution_count": 44
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 720x504 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = keras.Sequential([\n",
        "        keras.layers.Dense(28, input_dim=28, activation='relu'),\n",
        "        keras.layers.Dense(15, activation='relu'),\n",
        "        keras.layers.Dense(15, activation='relu'),\n",
        "        keras.layers.Dense(15, activation='relu'),\n",
        "        keras.layers.Dense(15, activation='relu'),\n",
        "        keras.layers.Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "\n",
        "model.fit(X_train_undersample,y_train_undersample, epochs=100)\n",
        "\n",
        "print(model.evaluate(X_test_undersample, y_test_undersample))\n",
        "\n",
        "y_preds = model.predict(X_test.values)\n",
        "y_preds = np.round(y_preds)\n",
        "\n",
        "print(\"Classification Report: \\n\", classification_report(y_test, y_preds))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ww5bEvGlAYE8",
        "outputId": "14bcbb7f-773a-4b28-d49a-4158374329c3"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "1/1 [==============================] - 1s 633ms/step - loss: 0.5615 - accuracy: 1.0000\n",
            "Epoch 2/100\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.5325 - accuracy: 1.0000\n",
            "Epoch 3/100\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.5041 - accuracy: 1.0000\n",
            "Epoch 4/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.4779 - accuracy: 1.0000\n",
            "Epoch 5/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.4531 - accuracy: 1.0000\n",
            "Epoch 6/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.4290 - accuracy: 1.0000\n",
            "Epoch 7/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.4043 - accuracy: 1.0000\n",
            "Epoch 8/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.3812 - accuracy: 1.0000\n",
            "Epoch 9/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.3593 - accuracy: 1.0000\n",
            "Epoch 10/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.3402 - accuracy: 1.0000\n",
            "Epoch 11/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.3216 - accuracy: 1.0000\n",
            "Epoch 12/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.3040 - accuracy: 1.0000\n",
            "Epoch 13/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.2874 - accuracy: 1.0000\n",
            "Epoch 14/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.2712 - accuracy: 1.0000\n",
            "Epoch 15/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.2555 - accuracy: 1.0000\n",
            "Epoch 16/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.2403 - accuracy: 1.0000\n",
            "Epoch 17/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.2256 - accuracy: 1.0000\n",
            "Epoch 18/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.2115 - accuracy: 1.0000\n",
            "Epoch 19/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.1980 - accuracy: 1.0000\n",
            "Epoch 20/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.1854 - accuracy: 1.0000\n",
            "Epoch 21/100\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.1733 - accuracy: 1.0000\n",
            "Epoch 22/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.1617 - accuracy: 1.0000\n",
            "Epoch 23/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.1508 - accuracy: 1.0000\n",
            "Epoch 24/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.1404 - accuracy: 1.0000\n",
            "Epoch 25/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.1305 - accuracy: 1.0000\n",
            "Epoch 26/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.1212 - accuracy: 1.0000\n",
            "Epoch 27/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.1125 - accuracy: 1.0000\n",
            "Epoch 28/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.1043 - accuracy: 1.0000\n",
            "Epoch 29/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.0966 - accuracy: 1.0000\n",
            "Epoch 30/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.0894 - accuracy: 1.0000\n",
            "Epoch 31/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.0828 - accuracy: 1.0000\n",
            "Epoch 32/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.0765 - accuracy: 1.0000\n",
            "Epoch 33/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.0707 - accuracy: 1.0000\n",
            "Epoch 34/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.0654 - accuracy: 1.0000\n",
            "Epoch 35/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.0604 - accuracy: 1.0000\n",
            "Epoch 36/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.0558 - accuracy: 1.0000\n",
            "Epoch 37/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.0515 - accuracy: 1.0000\n",
            "Epoch 38/100\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.0476 - accuracy: 1.0000\n",
            "Epoch 39/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.0440 - accuracy: 1.0000\n",
            "Epoch 40/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.0406 - accuracy: 1.0000\n",
            "Epoch 41/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.0376 - accuracy: 1.0000\n",
            "Epoch 42/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.0348 - accuracy: 1.0000\n",
            "Epoch 43/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.0322 - accuracy: 1.0000\n",
            "Epoch 44/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.0299 - accuracy: 1.0000\n",
            "Epoch 45/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.0277 - accuracy: 1.0000\n",
            "Epoch 46/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.0258 - accuracy: 1.0000\n",
            "Epoch 47/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.0240 - accuracy: 1.0000\n",
            "Epoch 48/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.0223 - accuracy: 1.0000\n",
            "Epoch 49/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.0208 - accuracy: 1.0000\n",
            "Epoch 50/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.0194 - accuracy: 1.0000\n",
            "Epoch 51/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.0182 - accuracy: 1.0000\n",
            "Epoch 52/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.0170 - accuracy: 1.0000\n",
            "Epoch 53/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.0159 - accuracy: 1.0000\n",
            "Epoch 54/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.0149 - accuracy: 1.0000\n",
            "Epoch 55/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.0139 - accuracy: 1.0000\n",
            "Epoch 56/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.0130 - accuracy: 1.0000\n",
            "Epoch 57/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.0121 - accuracy: 1.0000\n",
            "Epoch 58/100\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.0114 - accuracy: 1.0000\n",
            "Epoch 59/100\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.0107 - accuracy: 1.0000\n",
            "Epoch 60/100\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.0100 - accuracy: 1.0000\n",
            "Epoch 61/100\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.0094 - accuracy: 1.0000\n",
            "Epoch 62/100\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.0088 - accuracy: 1.0000\n",
            "Epoch 63/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.0083 - accuracy: 1.0000\n",
            "Epoch 64/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.0078 - accuracy: 1.0000\n",
            "Epoch 65/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.0074 - accuracy: 1.0000\n",
            "Epoch 66/100\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.0070 - accuracy: 1.0000\n",
            "Epoch 67/100\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.0066 - accuracy: 1.0000\n",
            "Epoch 68/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.0062 - accuracy: 1.0000\n",
            "Epoch 69/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.0059 - accuracy: 1.0000\n",
            "Epoch 70/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.0056 - accuracy: 1.0000\n",
            "Epoch 71/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.0053 - accuracy: 1.0000\n",
            "Epoch 72/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.0050 - accuracy: 1.0000\n",
            "Epoch 73/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.0048 - accuracy: 1.0000\n",
            "Epoch 74/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.0045 - accuracy: 1.0000\n",
            "Epoch 75/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.0043 - accuracy: 1.0000\n",
            "Epoch 76/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.0041 - accuracy: 1.0000\n",
            "Epoch 77/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.0039 - accuracy: 1.0000\n",
            "Epoch 78/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.0038 - accuracy: 1.0000\n",
            "Epoch 79/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.0036 - accuracy: 1.0000\n",
            "Epoch 80/100\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.0034 - accuracy: 1.0000\n",
            "Epoch 81/100\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.0033 - accuracy: 1.0000\n",
            "Epoch 82/100\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.0032 - accuracy: 1.0000\n",
            "Epoch 83/100\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.0030 - accuracy: 1.0000\n",
            "Epoch 84/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.0029 - accuracy: 1.0000\n",
            "Epoch 85/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.0028 - accuracy: 1.0000\n",
            "Epoch 86/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.0027 - accuracy: 1.0000\n",
            "Epoch 87/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.0026 - accuracy: 1.0000\n",
            "Epoch 88/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.0025 - accuracy: 1.0000\n",
            "Epoch 89/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.0024 - accuracy: 1.0000\n",
            "Epoch 90/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.0023 - accuracy: 1.0000\n",
            "Epoch 91/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.0023 - accuracy: 1.0000\n",
            "Epoch 92/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.0022 - accuracy: 1.0000\n",
            "Epoch 93/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.0021 - accuracy: 1.0000\n",
            "Epoch 94/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.0021 - accuracy: 1.0000\n",
            "Epoch 95/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.0020 - accuracy: 1.0000\n",
            "Epoch 96/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.0019 - accuracy: 1.0000\n",
            "Epoch 97/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.0019 - accuracy: 1.0000\n",
            "Epoch 98/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.0018 - accuracy: 1.0000\n",
            "Epoch 99/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.0018 - accuracy: 1.0000\n",
            "Epoch 100/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.0017 - accuracy: 1.0000\n",
            "1/1 [==============================] - 0s 476ms/step - loss: 0.2715 - accuracy: 1.0000\n",
            "[0.2714639902114868, 1.0]\n",
            "Classification Report: \n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "         0.0       1.00      1.00      1.00       864\n",
            "         1.0       0.00      0.00      0.00         1\n",
            "\n",
            "    accuracy                           1.00       865\n",
            "   macro avg       0.50      0.50      0.50       865\n",
            "weighted avg       1.00      1.00      1.00       865\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Well, congratulation!! We just received 99.95% approx accuracy in our credit card fraud detection. This number should not be surprising as our data was balanced towards one class. The good thing that we have noticed from the confusion matrix is that — our model is not overfitted."
      ],
      "metadata": {
        "id": "nwuYJ5zPJ02G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **THANK YOU........))**"
      ],
      "metadata": {
        "id": "MWLjdnaKKEhX"
      }
    }
  ]
}